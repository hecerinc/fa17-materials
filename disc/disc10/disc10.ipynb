{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra and gradient review\n",
    "\n",
    "In this section. We will review concepts of linear algebra and gradient using Ridge regression as an example.\n",
    "\n",
    "Material to cover:\n",
    "1. Rank of the matrix\n",
    "2. Inverse of the the matrix\n",
    "3. properties of $X^TX$\n",
    "4. Inner and outer product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank\n",
    "[Rank] is the maximum number of linearly ***independent vectors*** in a matrix. \n",
    "[Rank]: https://en.wikipedia.org/wiki/Rank_(linear_algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [[1,1],\n",
    "     [2,3]]\n",
    "b = [[1,1],\n",
    "     [2,2]]\n",
    "c = [[1,0],\n",
    "     [0,0]]\n",
    "np.linalg.matrix_rank(a),np.linalg.matrix_rank(b),np.linalg.matrix_rank(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of the matrix\n",
    "$XX^{-1} = I$\n",
    "\n",
    "X must be square and full-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(a)  \n",
    "#np.linalg.inv(b)  Will get error as singular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a).dot(np.linalg.inv(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = [[1,0,0],\n",
    "        [0,3,0],\n",
    "        [0,0,5]]\n",
    "np.linalg.inv(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diag = [[1,0,0],\n",
    "        [0,3,0],\n",
    "        [0,0,0]]\n",
    "#np.linalg.inv(diag) ###You can't do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse of a diagonal matrix is just the inverse of each value. Therefore there is no inverse for a matrix that is not full-rank\n",
    "\n",
    "Also, `np.linalg.solve` is doing similar things as inverse.\n",
    "\n",
    "$Ax=\\beta$\n",
    "\n",
    "$x=A^{-1}\\beta$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=[2,3]\n",
    "np.linalg.solve(a,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(a).dot(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $X^TX$\n",
    "We use $X^TX$ a lot in linear regression. Since it will be always a square matrix. And it has the same rank as X.\n",
    "\n",
    "Why?\n",
    "Since $Xv=0$ if and only if $X^TXv=0$ , and then follow some maths about [null space] and [Rankâ€“nullity theorem](https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem)\n",
    "\n",
    "[null space]: https://en.wikipedia.org/wiki/Kernel_(linear_algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[[1,2],\n",
    "  [2,4],\n",
    "  [5,6]]\n",
    "e=[[1,1],\n",
    "  [2,2],\n",
    "  [3,3]]\n",
    "np.linalg.matrix_rank(d),np.linalg.matrix_rank(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.inv(d)\n",
    "dd=np.array(d).transpose().dot(d)\n",
    "ee=np.array(e).transpose().dot(e)\n",
    "np.linalg.matrix_rank(dd),np.linalg.matrix_rank(ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $X^TX$ is not always invertible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer and inner product\n",
    "Suppose X is row vector.\n",
    "$X^TX$ is outer product, $XX^T$ is inner product. But in numpy, there is difference between 1-D array and 2-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer(v,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(v,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.transpose().dot(v) #Transpose doesn't work for 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv=np.array([[1,2,3]])\n",
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.transpose().dot(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.dot(vv.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ridge Regression\n",
    "\n",
    "There are many forms for $\\textbf{R}(\\theta)$ but a common form is the squared **$L^2$** norm of $\\theta$.\n",
    "\n",
    "$$\\large\n",
    "\\large \\textbf{R}_{L^2}(\\theta) = \n",
    "\\large||\\theta||_2^2 = \\theta^T \\theta  = \\sum_{k=1}^p \\theta_k^2\n",
    "$$\n",
    "\n",
    "In the context of least squares regression this is often referred to as **Ridge Regression** with the objective:\n",
    "\n",
    "$$ \\large\n",
    "\\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f_\\theta(x_i)\\right)^2 + \\lambda ||\\theta||_2^2\n",
    "$$\n",
    "\n",
    "This is also sometimes called [Tikhonov Regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the optimal $\\hat{\\theta}$ with $L^2$ Regularization\n",
    "\n",
    "We return to our linear model formulation:\n",
    "\n",
    "$$ \\large\n",
    "f_\\theta(x) = x^T \\theta\n",
    "$$\n",
    "\n",
    "Using the standard matrix notation:\n",
    "\n",
    "<img src=\"images/matrix_dot.png\" width=\"400px\">\n",
    "\n",
    "We can rewrite the objection\n",
    "\n",
    "\n",
    "\\begin{align}\\large\n",
    "\\hat{\\theta}_{\\text{L2}} = \\arg\\min_\\theta \\frac{1}{n}\\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right)  + \\lambda \\theta^T \\theta\n",
    "\\end{align}\n",
    "\n",
    "Expanding the objective term:\n",
    "\n",
    "\\begin{align}\\large\n",
    "L_\\lambda(\\theta) = \\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right)  + \\lambda \\theta^T \\theta = \n",
    "\\frac{1}{n} \\left( \n",
    " Y^T Y -  2 Y^T X \\theta + \\theta^T  X^T  X \\theta \n",
    "\\right) + \\lambda \\theta^T \\theta\n",
    "\\end{align}\n",
    "\n",
    "Taking the **gradient** with respect to $\\theta$:\n",
    "\n",
    "\n",
    "\\begin{align} \\large\n",
    "\\nabla_\\theta L_\\lambda(\\theta)\n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " \\nabla_\\theta Y^T Y -  \\nabla_\\theta 2 Y^T X \\theta + \\nabla_\\theta \\theta^T  X^T  X \\theta \n",
    "\\right) + \\nabla_\\theta  \\lambda \\theta^T \\theta \\\\\n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " 0 -  2 X^T Y  +  2 X^T  X \\theta \n",
    "\\right) + 2\\lambda \\theta\n",
    "\\end{align} \n",
    "\n",
    "The above gradient derivation uses the following identities:\n",
    "1. $\\large \\nabla_\\theta \\left( A \\theta  \\right) = A^T$\n",
    "1. $\\large \\nabla_\\theta \\left( \\theta^T A \\theta \\right) = A\\theta + A^T \\theta$ and $\\large A = X^T X$ is symmetric\n",
    "\n",
    "Setting the gradient equal to zero we get a **regularized** version of the **normal equations**:\n",
    "\n",
    "$$\\large\n",
    "(X^T  X  + n \\lambda I) \\theta =  X^T Y\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    " \\theta = \\left(X^T  X  + n \\lambda I \\right)^{-1} X^T Y\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Optimal $\\theta$ under $L^2$ regularization\n",
    "\n",
    "\n",
    "Because $\\lambda$ is a tuning parameter we often will absorb the $n$ into $\\lambda$ and rewrite the above equations as:\n",
    "\n",
    "\n",
    "\n",
    "$$\\large\n",
    "(X^T  X  + \\lambda I) \\theta =  X^T Y\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    " \\theta = \\left(X^T  X  + \\lambda I \\right)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "**Notice:** The addition of $\\lambda I$ ensures that $X^T  X  + \\lambda I$ is **full rank**.  This addresses the earlier issue in least-squares regression when we had co-linear features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How does $L^2$ Regularization Help\n",
    "\n",
    "The $L^2$ penalty helps in several ways:\n",
    "\n",
    "**Manages Model Complexity**\n",
    "1. It ensures that uninformative features weights are relatively small (near zero) mitigating the affect of those features.  \n",
    "1. It evenly distributes weight over similar features to reduce variance.\n",
    "\n",
    "**Practical Concerns**\n",
    "1. It removes degeneracy created by co-linear features\n",
    "1. It improves the numerical stability of\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Normalization and the Intercept\n",
    "\n",
    "Before we proceed it is important that we appropriately normalize the data.  Because the standard $L^2$ regularization methods treat each dimensional equivalently it is important that all dimensions are in the same range of values.  \n",
    "\n",
    "However, we notice that the distribution of values \n",
    "can be quite different for each dimension.\n",
    "\n",
    "For example in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "_,_,a=plt.hist(df[\"carat\"],bins=30)\n",
    "plt.subplot(2,2,2)\n",
    "_,_,a=plt.hist(df[\"depth\"],bins=30)\n",
    "plt.subplot(2,2,3)\n",
    "_,_,a=plt.hist(df[\"table\"],bins=30)\n",
    "plt.subplot(2,2,4)\n",
    "_,_,a=plt.hist(df[\"price\"],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[[\"carat\",\"depth\",\"table\"]])\n",
    "Y = np.array(df[\"price\"])\n",
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "lm.coef_,lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=linear_model.Ridge(alpha=1)\n",
    "r.fit(X,Y)\n",
    "r.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions in the homework\n",
    "Q: Can we say carat is the dominating feature?\n",
    "\n",
    "A: We can't make the judgement based on the scale of the coefficent when the data is not normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the Data\n",
    "\n",
    "A common transformation is to center and scale the features to zero mean and unit variance:\n",
    "\n",
    "$$\\large\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "This an be accomplished by applying the `StandardScalar` scikit learn preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "normalizer.fit(X)\n",
    "X_norm=normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "_,_,a=plt.hist(X_norm[:,0],bins=30)\n",
    "plt.subplot(2,2,2)\n",
    "_,_,a=plt.hist(X_norm[:,1],bins=30)\n",
    "plt.subplot(2,2,3)\n",
    "_,_,a=plt.hist(X_norm[:,2],bins=30)\n",
    "plt.subplot(2,2,4)\n",
    "_,_,a=plt.hist(df[\"price\"],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=linear_model.Ridge(alpha=1)\n",
    "r.fit(normalizer.transform(X),Y)\n",
    "r.coef_,r.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"carat\"],df[\"price\"],\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we say about carat?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "336px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}